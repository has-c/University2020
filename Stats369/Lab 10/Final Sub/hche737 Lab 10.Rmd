---
title: "Lab 10"
author: "Hasnain Cheena"
date: "26/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)

#libraries from word cloud demo
library(readr)
library(dplyr)
library(e1071)
library(mlbench)
 
#Text mining packages
library(tm)
library(SnowballC)
library("wordcloud")
library("RColorBrewer")

set.seed(369)
```

## Question 1

Load the dataset and explore the frequency of text sentence by (key) characters. Find the top 5 characters who say the most lines of texts. Use suitable graphs to show their frequency count change over each season.

```{r Load in GOT data, cache=TRUE, message=FALSE}
base_path = "C:\\Users\\Hasnaij\\Desktop\\University2020\\Stats369\\Lab 10\\"
got_df = read_csv(file.path(base_path, "Game_of_Thrones_Script.csv"))

#Top 5 characters how say the most lines of text
top_5_characters = got_df %>% 
  group_by(Name) %>%
  tally(name="dialog_count", sort=TRUE) %>%
  head(5)
top_5_characters

#how frequency count changes over each season
search_mask = got_df$Name %in% top_5_characters$Name #find where the top 5 characters have spoke
top_5_got_df = got_df[search_mask, ] %>%
  mutate(Name=factor(Name),
         Season_fact = factor(Season),
         Season_no = recode(Season_fact,
                      "Season 1" = "1",
                      "Season 2" = "2",
                      "Season 3" = "3",
                      "Season 4" = "4",
                      "Season 5" = "5",
                      "Season 6" = "6",
                      "Season 7" = "7",
                      "Season 8" = "8"))
#plot
top_5_got_df %>%
  group_by(Season_no, Name) %>%
  tally(name="dialog_count") %>%
  ggplot(aes(x=Season_no, y=dialog_count)) +
  geom_point() +
  facet_wrap(~Name, nrow=3) +
  labs(title="Top 5 GOT Characters Dialog Frequency Count per Season", x="Season Number", y="Number of lines")
  


```

## Question 2
Use GloVe embedding with glmnet to produce a sentiment score for each word. Roll up the sentiment scores – by taking average over all the words in a sentence – to get an overall sentiment for each line of text. You can then append this ‘Sentiment’ attribute to the raw data set.

Produce a picture showing the change in sentiments by season 1 episodes for the 5 characters identified from Q1.

#### Load in Data
Load in data and perform sentiment analysis. 
```{r Load Data, cache=TRUE, eval=FALSE}
#GLOVE
con = dplyr::src_sqlite(file.path(base_path, 'words2.db')) # specify db name
glove = tbl(con, 'glove') # table name

#GOOD and BAD words
pos_words <- scan(file.path(base_path, 'positive-words.txt'),
blank.lines.skip = TRUE, comment.char = ";" , what = "")
neg_words <- scan(file.path(base_path, 'negative-words.txt'),
blank.lines.skip = TRUE, comment.char = ";" , what = "")

# bag those into training words. 1: positive; 0: negative
train_words <- tibble(word = c(pos_words, neg_words),
                      pos = rep(1:0, c(length(pos_words),
                                       length(neg_words))))

# copy to db
train_words = copy_to(con, train_words, name = 'train_words' ,temporary = TRUE)

train.df = inner_join(train_words, glove, by = c('word' = 'Word')) %>% collect() # joining with glove

# create design matrix for 'glmnet'
train_x <- train.df %>%
select(-word, -pos) %>%
as.matrix()
rownames(train_x) <- train.df$word
# response
train_y <- train.df$pos

#fit logistic model for classification with a mixed of L1 and L2 penalty
test <- sample(nrow(train_x), nrow(train_x)/5) # 20% as test data
fit <- cv.glmnet(train_x[-test,], train_y[-test], family = 'binomial', alpha = 0.5)

```

Create function to return sentiment of text. 
```{r Sentiment Function, cache=TRUE, eval=FALSE}
predict_sentiment <- function(db_con = con,
                              db_name = 'glove',
                              text,
                              model = fit) {
  
  glove <- tbl(db_con, db_name)
  word_tbl <- copy_to(
    db_con,
    tibble(word =
             tolower(
               strsplit(text,
                        "[[:blank:],.!?;:'\"]")[[1]]
             )),
    name = "temp_words"
    ,
    overwrite = TRUE,
    temporary = TRUE
  
    )
  word_x <- inner_join(word_tbl, glove, by = c('word' = 'Word')) %>%
    collect() %>%
    select(-word) %>%
    as.matrix()
  if (nrow(word_x) == 0)
    return(0)
  senti <- predict(model$glmnet.fit, word_x, s = model$lambda.min)
  return(mean(senti))
}
```

#### Sentiment analysis on GOT Data
Perform sentiment analysis. As this bit took a long time to run, save the output and read it in to plot the data. 
```{r Sentiment GOT data, cache=TRUE, message=FALSE, eval = FALSE}
top_5_got_df$raw_polarity_score = 0

for (idx in 1:nrow(top_5_got_df)){
  text = top_5_got_df$Sentence[idx]
  sentiment = predict_sentiment(text = text)
  top_5_got_df$raw_polarity_score[idx] = sentiment
}

write_csv(top_5_got_df, "lab 10 sentiment.csv")
```

#### Plot Data
```{r Plot Sentiment Data, cache=TRUE, message=FALSE, warning=FALSE}
got_sentiment_df = read_csv("lab 10 sentiment.csv")

avg_sentiment_df = got_sentiment_df %>%
  filter(Season == "Season 1") %>%
  mutate(ep_factor = factor(Episode),
         actor_factor = factor(Name),
          ep_factor = recode(ep_factor,
              "Episode 1" = "1",
              "Episode 2" = "2",
              "Episode 3" = "3",
              "Episode 4" = "4",
              "Episode 5" = "5",
              "Episode 6" = "6",
              "Episode 7" = "7",
              "Episode 8" = "8",
              "Episode 9" = "9",
              "Episode 10" = "10")) %>%
  group_by(ep_factor, actor_factor) %>%
  summarise(avg_sentiment = mean(raw_polarity_score, na.rm=TRUE))

#reorder levels to get eps in correct order
ep_levels = levels(avg_sentiment_df$ep_factor)
level_order = c(1, as.numeric(ep_levels[3:length(ep_levels)]), 10)
avg_sentiment_df$ep_factor = factor(avg_sentiment_df$ep_factor, levels=level_order)

avg_sentiment_df %>%
  ggplot(aes(x=ep_factor, y=avg_sentiment)) +
  geom_point() +
  facet_wrap(~actor_factor, nrow=3) +
  labs(title="Top 5 GOT Characters Average Sentiment per Episode", x="Episode Number", y="Average Sentiment")
```

## Question 3
Pick a character from Q1, use the sentiment scores generated from Q2 to make a graph showing the fluctuation of sentiment scores for all the texts associated with that character over each season & episode. Answer the question based on the picture produced.

I have selected Tyrion Lannister. 

```{r Plot Sentiment Data Tyrion, cache=TRUE, message=FALSE, warning=FALSE}
got_sentiment_df = read_csv("lab 10 sentiment.csv")

tyrion_sentiment_df = got_sentiment_df %>%
  filter(Name == "tyrion lannister") %>%
  mutate(ep_factor = factor(Episode),
         season_factor = factor(Season),
         season_factor = recode(season_factor,
                      "Season 1" = "1",
                      "Season 2" = "2",
                      "Season 3" = "3",
                      "Season 4" = "4",
                      "Season 5" = "5",
                      "Season 6" = "6",
                      "Season 7" = "7",
                      "Season 8" = "8"),
          ep_factor = recode(ep_factor,
              "Episode 1" = "1",
              "Episode 2" = "2",
              "Episode 3" = "3",
              "Episode 4" = "4",
              "Episode 5" = "5",
              "Episode 6" = "6",
              "Episode 7" = "7",
              "Episode 8" = "8",
              "Episode 9" = "9",
              "Episode 10" = "10")) %>%
  group_by(season_factor, ep_factor) %>%
  summarise(avg_sentiment = mean(raw_polarity_score, na.rm=TRUE))

ep_levels = levels(tyrion_sentiment_df$ep_factor)
level_order = c(1, as.numeric(ep_levels[3:length(ep_levels)]), 10)
tyrion_sentiment_df$ep_factor = factor(tyrion_sentiment_df$ep_factor, levels=level_order)

tyrion_sentiment_df %>%
  ggplot(aes(x=ep_factor, y=avg_sentiment)) +
  geom_point() +
  facet_wrap(~season_factor, nrow=5) +
  labs(title="Tyrion Lannister Average Sentiment per Episode and per Season", x="Episode Number", y="Average Sentiment")
```

No clear pattern with Tyrion.He seems to start every season at neutral sentiment and then typically drops to negative sentiment between episodes 2-4. Then after that he generally begins cheer up again (rises to positive sentiment) and then levels off towards the end of the season on neutral again. 

## Question 4
For the character you chose from Q3, produce a word cloud showing the most frequently used words spoken by that character.
Again I selected Tyrion Lannister. 

```{r Plot Word Cloud, cache=TRUE, message=FALSE, warning=FALSE}
got_sentiment_df = read_csv("lab 10 sentiment.csv")

tyrion_sentiment_df = got_sentiment_df %>%
  filter(Name == "tyrion lannister")

corpus = Corpus(VectorSource(tyrion_sentiment_df$Sentence))

#text processing
#Conversion to Lowercase
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, tolower)
#Removing Punctuation
corpus = tm_map(corpus, removePunctuation)
#Remove stopwords
corpus = tm_map(corpus, removeWords, c("cloth", stopwords("english")))
# Stemming
corpus = tm_map(corpus, stemDocument)
# Eliminate white spaces
corpus = tm_map(corpus, stripWhitespace)
      

#convert to tdm
DTM <- TermDocumentMatrix(corpus)
mat <- as.matrix(DTM)
f <- sort(rowSums(mat),decreasing=TRUE)
dat <- data.frame(word = names(f),freq=f)

wordcloud(words = dat$word, freq = dat$freq, random.order=TRUE)
```

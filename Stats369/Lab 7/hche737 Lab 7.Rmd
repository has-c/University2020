---
title: 'Lab 7: Spam with Random Forests'
author: "Hasnain Cheena"
date: "06/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ranger)
library(tidyverse)

set.seed(369)
```

```{r Read in Data}
load("spam.rda")

#create feature-label df
word.df = data.frame(wordmatrix)
col.names = make.names(names(word.df))
names(word.df) = col.names
spam.df = cbind(word.df, df['is_spam'])
```

## Task 1
Fit random forests with 1,2,…,9,10,20,…,50…90,100, 200,..,600 trees.
Repeat, to get two of each size.

```{r Task 1}
forest.size.df = data.frame()
sizes = rep(c(seq(1,9), seq(10,50,10), seq(100,600,100)),2)

for(size in sizes){
  
  spam.forest = ranger(is_spam~. , data=spam.df, importance="impurity", num.trees = size)
  forest.size.df = rbind(forest.size.df, c(size, spam.forest$prediction.error))
  
}
names(forest.size.df) = c("size", "OOB Error Rate")
```

## Task 2
Plot the OOB error rate against the number of trees.

```{r Task 2, message=FALSE, fig.width=12, fig.height=6}
forest.size.df %>%
  group_by(size) %>%
  summarise("Avg OOB Error Rate" = mean(`OOB Error Rate`)) %>%
  ggplot(aes(x=size, y=`Avg OOB Error Rate`)) + geom_point() + geom_line() + labs(title = "OOB Error Rate vs Size", y="OOB Error Rate", x="Size")
```

From the plot above it seems that 500 trees per forest results in the best OOB rate. 

## Task 3
With the default number of trees, grow forests with mtry= 2,5,10,20,30,40,50,200.

```{r Task 3}
forest.predictor.numbers.df = data.frame()
mtry.values = c(2,5,10,20,30,40,50,200)

for(mtry in mtry.values){
  
  spam.forest = ranger(is_spam~. , data=spam.df, importance="impurity", mtry=mtry)
  forest.predictor.numbers.df = rbind(forest.predictor.numbers.df, c(mtry, spam.forest$prediction.error))
  
}

names(forest.predictor.numbers.df) = c("mtry", "OOB Error Rate")
```

## Task 4
Plot the OOB error rate against mtry. What is the default value for mtry and is it a reasonable choice in this case?

```{r Task 4, fig.width=12, fig.height=6}
forest.predictor.numbers.df %>%
  ggplot(aes(x=mtry, y=`OOB Error Rate`)) + geom_point() + geom_line() + labs(title = "OOB Error Rate vs Number of Predictors at each split", y="OOB Error Rate", x="Number of Predictors at each node")
```

The default value of $mtry$ is the rounded down square root of the number of variables. In this case that value is 25 $(\sqrt{630} \approx 25)$.
It is a reasonable choice in this case as the OOB Error rate is at a minimum between mtry values of 20 and 30 .

## Task 5

How do the results compare to A3 rpart? 

```{r Confusion Matrix Comparision, message=FALSE}
#RANDOM FOREST
#retrain using best values
spam.forest = ranger(is_spam~. , data=spam.df, importance="impurity", num.trees = 500)
#confusion matrix for forest
spam.df$pred = recode(spam.forest$predictions, "0"=FALSE, "1"=TRUE)
with(spam.df, table(actual=is_spam, predictions = pred))

#RPART DECISION TREE FROM A3
rpart_pred_A3 = read_csv("Lab 7 Rpart.csv")
table(actual=spam.df$is_spam, predictions = rpart_pred_A3$r_part_pred)
```

The Random Forest generated in lab 7 is able to classify many more observations correctly (thus has a higher accuracy) than the decision tree created in A3.Therefore, the Random Forest is a better predictor of spam/not-spam than the Decision Tree. This can be seen in the confusion matrices generated by both models. 
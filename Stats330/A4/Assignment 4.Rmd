---
title: "Stats 330 Assignment 4"
author: "Hasnain Cheena"
date: "04/06/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

### 1a)
```{r Q1a - P1}
feuro.df = read.csv('feuro.csv', header = TRUE)

#create BMI
feuro.df <- transform(feuro.df, BMI = weight / height^2)

# Sort by age
sort.order <- with(feuro.df, order(age))
feuro.df <- feuro.df[sort.order, ]

#fit linear model with quadratic in age
quadbmi.fit = lm(BMI ~ poly(age, 2, raw=TRUE), feuro.df)
```
To obtain the age at which BMI is maximised: 

$$ ax^2 + bx + c$$ 

$$ \frac{d}{dx} (ax^2 + bx + c) = 0$$

$$ 2ax + b = 0$$

$$ x = \frac{-b}{2a}$$

```{r Q1a - P2}
#obtain age where bmi is maximised
theta.hat = (-1*quadbmi.fit$coefficients[2])/(2*quadbmi.fit$coefficients[3])
theta.hat
``` 

Therefore the estimated age at which BMI is maximised for European women is 61.9 years old.


```{r Q1a - P3, fig.width=10, fig.height=6}
azure  <- "#007FFF"

plot(BMI ~ jitter(age), data = feuro.df, col = azure, cex = 0.5,las = 1, main = "European-type Females",  ylab = "BMI (kg/m^2)")
lines(with(feuro.df, age), fitted(quadbmi.fit), col = "red", lwd=2)
``` 

### Q1b
```{r Q1b - Parametric Bootstrapping}
#reproducible
set.seed(1688)

n.sims = 10000
#create vectors to store estimates
est.theta = numeric(n.sims)
#number of observations
n = nrow(feuro.df)
#calculate expected values
feuro.df$expected.values = quadbmi.fit$coefficients[1] + quadbmi.fit$coefficients[2]*feuro.df$age + quadbmi.fit$coefficients[3]*(feuro.df$age)^2

sigma.hat = summary(quadbmi.fit)$sigma

#simulate
for (i in 1:n.sims){
  #simulate responses
  bmi.simulated = rnorm(n, feuro.df$expected.values, sigma.hat)
  
  #refit
  sim.fit = lm(bmi.simulated ~ poly(feuro.df$age, 2, raw=TRUE))
  
  #calculate theta and save
  est.theta[i] = (-1*sim.fit$coefficients[2])/(2*sim.fit$coefficients[3])
}

#quantiles to obtain 95% interval
a = theta.hat - quantile(est.theta, prob=0.025)
b = quantile(est.theta, prob=0.975) - theta.hat
c(theta.hat-b, theta.hat+a)
```

### Q1c
```{r Q1c - Non-parametric Bootstrapping}
#reproducible
set.seed(1688)

n.sims = 10000
#create vectors to store estimates
est.theta = numeric(n.sims)
#number of observations
n = nrow(feuro.df)

#simulate
for (i in 1:n.sims){
  #resample dataframe
  samp = sample(1:n, replace=TRUE)
  boot.df = feuro.df[samp,]
  
  #fit the model
  sim.fit = lm(BMI ~ poly(age, 2, raw=TRUE), boot.df)
  
  #calculate theta and save
  est.theta[i] = (-1*sim.fit$coefficients[2])/(2*sim.fit$coefficients[3])
}

#quantiles to obtain 95% interval
a = theta.hat - quantile(est.theta, prob=0.025)
b = quantile(est.theta, prob=0.975) - theta.hat
c(theta.hat-b, theta.hat+a)
```

Compare 95% parametric to non-parametric 

### Q1d
```{r Q1d - Delta method}
library(msm)

#using original model fit from abv
quadbmi.fit = lm(BMI ~ poly(age, 2, raw=TRUE), feuro.df)

#dm 
dm.sd = deltamethod(~ -x1/(2*x2), coef(quadbmi.fit), vcov(quadbmi.fit))
(dm.CI = -coef(quadbmi.fit)[2]/(2*coef(quadbmi.fit)[3]) + 1.96 * c(-1,1) * dm.sd)
```

Compare answers to non-parametric and parametric bootstrapping

### Q1e



## Question 2
```{r Q2a}
library(datasets)
library(MuMIn)

fm1 <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)

#use dredge to get a model selected from a table of models
#only keep models that include models with ddpi and use BIC to rank models
options(na.action="na.fail", width=120)
dredge(fm1,rank="BIC", subset=with(ddpi))
```

## Question 3

### Q3a
```{r Q3a}
# Generate the 'original' data
#ID: 190411106
set.seed(267)

n <- 100
X <- scale(3 * (1:n)/n, scale = FALSE)
myfun <- function(x)
  2 - x + 3*x*x
Y <- myfun(X) + rnorm(n)

plot(X, Y, col = "blue")
fit <- smooth.spline(X, Y, df = 3 , all = TRUE)
lines(fit, lty = 1, col = "darkgreen", lwd = 2)
```
Comment


### Q3b
```{r Q3b}
#smooth spline with df=2
spline.df2 <- smooth.spline(X, Y, df = 2 , all = TRUE)

#smooth spline with df=20
spline.df20 <- smooth.spline(X, Y, df = 20 , all = TRUE)

plot(X, Y, col = "blue")
lines(fit, lty = 1, col = "darkgreen", lwd = 2)
lines(spline.df2, lty = 1, col = "red", lwd = 2)
lines(spline.df20, lty = 1, col = "yellow", lwd = 2)

```

Comment on clear underfitting overfitting etc

### Q3c
```{r Q3c }

df = 2:n
results.df = data.frame(df=df, MSE= matrix(0, nrow=n-1))


for (i in df){
  #fit spline
  smooth.fit = smooth.spline(X, Y, df = i , all = TRUE)
  #calculate MSE and add to df
  results.df$MSE[i-1] = (sum((Y - smooth.fit$y)^2))/n
}

#plot using line
plot(df, MSE, type="n") 
lines(df, MSE,  lty = 1, col = "red", lwd = 2)
```
Comment about overfitting and underfitting and major drop initiality and inflection point (Where adding more complexity is not helping)
Generalisation and onverfitting

### Q3d
```{r Q3d}
#model complexity vs prediction error
#generate new test data
n <- 100
X.test <- scale(3 * (1:n)/n, scale = FALSE)
myfun <- function(x)
  2 - x + 3*x*x
Y.test <- myfun(X.test) + rnorm(n)

df = 2:n
results.train.df = data.frame(df=df, MSE= matrix(0, nrow=n-1))
results.test.df = data.frame(df=df, MSE= matrix(0, nrow=n-1))

#fit splines with training data and fit on test data
for (i in df){
  #fit spline
  smooth.fit = smooth.spline(X, Y, df = i , all = TRUE)
  #calculate MSE and add to training df
  results.train.df$MSE[i-1] = (sum((Y - smooth.fit$y)^2))/n
  
  #predict test 
  Y.pred = predict(smooth.fit, data=X.test, type="response")
  #calculate MSE
  results.test.df$MSE[i-1] = (sum((Y.test - Y.pred$y)^2))/n
}

#plot using line
plot(df, MSE, type="n") 
lines(MSE~df,  lty = 1, col = "red", lwd = 2, results.train.df)
lines(MSE~df,  lty = 1, col = "blue", lwd = 2, results.test.df)
```


### Q3e
```{r Q3e}
smooth.autogcv.fit = smooth.spline(X, Y, all = TRUE)
smooth.autogcv.fit

#plot a scatter plot with the smoother
plot(X, Y, col = "blue")
lines(smooth.autogcv.fit, lty = 1, col = "pink", lwd = 2)
```
Comment


### Q3f


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from preprocessing import normalize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Naive Bayes Data.xlsx\")\n",
    "\n",
    "features = df['abstract']\n",
    "labels = df.loc[:3]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['kiwi sheep kiwi', 'kiwi kiwi bird', 'kiwi auckland',\n",
       "       'munich oktoberfest kiwi', 'kiwi kiwi kiwi munich oktoberfest'],\n",
       "      dtype='<U33')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize and tokenize then restring text\n",
    "cleaned_features = list()\n",
    "for feature in features:\n",
    "    words = word_tokenize(feature)\n",
    "    normalised_words = normalize(words)\n",
    "    abstract = \" \".join(normalised_words)\n",
    "    cleaned_features.append(abstract)\n",
    "    \n",
    "features = np.array(cleaned_features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NZ': 0.75, 'DE': 0.25}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate priors\n",
    "priors = dict()\n",
    "unique_classes = pd.unique(labels)\n",
    "for label in unique_classes:\n",
    "    priors[label] = np.sum(label == labels)/len(labels)\n",
    "    \n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditional probabilities\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#features_count: rows = number of documents, columns = unique word counts\n",
    "features_count = count_vect.fit_transform(features)\n",
    "\n",
    "feature_mapping = count_vect.vocabulary_ #words mapped to column indexes\n",
    "unique_words = np.array(list(feature_mapping.keys())) #unique words in the documents\n",
    "\n",
    "#using test words to calculate conditional probs\n",
    "train_features = features_count[:4, :]\n",
    "test_features = features_count[4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditional prob matrix\n",
    "# row = label\n",
    "# col = features\n",
    "\n",
    "number_of_words = len(unique_words) # |V|\n",
    "\n",
    "#calculate count(c)\n",
    "number_of_words_per_class = dict() #class is the key and value is the number of unique words\n",
    "for label in labels:\n",
    "    #subset by class\n",
    "    subset_features = train_features.toarray()[label == labels]\n",
    "    \n",
    "    word_count = 0\n",
    "    for abstract in subset_features:\n",
    "        word_count += np.sum(abstract)\n",
    "    \n",
    "    number_of_words_per_class[label] = word_count\n",
    "\n",
    "conditional_prob = pd.DataFrame()\n",
    "\n",
    "#calculate count(w,c) and calculate conditional prob\n",
    "for label in unique_classes:\n",
    "    \n",
    "    row = list()\n",
    "    for word in unique_words:\n",
    "        \n",
    "        \n",
    "        \n",
    "        #subset by class\n",
    "        subset_features = train_features.toarray()[label == labels]\n",
    "        count_of_word_in_class = np.sum(subset_features[:,feature_mapping[word]]) #count(w,c)\n",
    "        \n",
    "        #P(w|c)\n",
    "        prob = (count_of_word_in_class + 1)/(number_of_words_per_class[label] + number_of_words)\n",
    "        \n",
    "        row.append(prob)\n",
    "        \n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42857142857142855,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-b610d2265f4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "unique_words[test_features > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.toarray() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['kiwi', 'sheep', 'bird', 'auckland', 'munich', 'oktoberfest'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird', 'oktoberfest', 'sheep', 'kiwi', 'auckland', 'munich']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kiwi': 2,\n",
       " 'sheep': 5,\n",
       " 'bird': 1,\n",
       " 'auckland': 0,\n",
       " 'munich': 3,\n",
       " 'oktoberfest': 4}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
